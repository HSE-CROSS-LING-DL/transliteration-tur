{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "calma_crh_all.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bjzsghUSHgBZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### set up telegram notifications\n",
        "\n",
        "  не очень понятно, нужно ли это.\n",
        "\n",
        "  если нужно -- напишите @oserikov в телеграме, я расскажу, что сделать,\n",
        "  чтобы присылались сообщения с качеством модели когда она отработает."
      ]
    },
    {
      "metadata": {
        "id": "ouaEI2LJHgBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "telegram_notifications_enabled=False\n",
        "EXP_DESCRIPTION = \"BASELINE\"\n",
        "\n",
        "if telegram_notifications_enabled:\n",
        "    bot_token = input(\"введите telegram bot token: \")\n",
        "    chat_id = \"292749902\" # for @oserikov\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8GCVupLxHgBn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " #### install prereqs"
      ]
    },
    {
      "metadata": {
        "id": "I7uH7hxlHgBr",
        "colab_type": "code",
        "outputId": "d31e877d-4a81-44fe-cc3c-b0657e159658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        }
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system(f\"git clone https://github.com/NIS-2018-CROSS-M/colab-tools.git\")\n",
        "get_ipython().magic(f\"cd colab-tools\")\n",
        "get_ipython().system(f\"bash colab-install-opennmt.sh\")\n",
        "get_ipython().system(f\"bash colab-install-cuda92-pytorch41.sh\")\n",
        "get_ipython().system(f\"bash colab-install-torchtext.sh\")\n",
        "get_ipython().magic(f\"cd ..\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab-tools'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects:   7% (1/14)   \u001b[K\rremote: Counting objects:  14% (2/14)   \u001b[K\rremote: Counting objects:  21% (3/14)   \u001b[K\rremote: Counting objects:  28% (4/14)   \u001b[K\rremote: Counting objects:  35% (5/14)   \u001b[K\rremote: Counting objects:  42% (6/14)   \u001b[K\rremote: Counting objects:  50% (7/14)   \u001b[K\rremote: Counting objects:  57% (8/14)   \u001b[K\rremote: Counting objects:  64% (9/14)   \u001b[K\rremote: Counting objects:  71% (10/14)   \u001b[K\rremote: Counting objects:  78% (11/14)   \u001b[K\rremote: Counting objects:  85% (12/14)   \u001b[K\rremote: Counting objects:  92% (13/14)   \u001b[K\rremote: Counting objects: 100% (14/14)   \u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects:   8% (1/12)   \u001b[K\rremote: Compressing objects:  16% (2/12)   \u001b[K\rremote: Compressing objects:  25% (3/12)   \u001b[K\rremote: Compressing objects:  33% (4/12)   \u001b[K\rremote: Compressing objects:  41% (5/12)   \u001b[K\rremote: Compressing objects:  50% (6/12)   \u001b[K\rremote: Compressing objects:  58% (7/12)   \u001b[K\rremote: Compressing objects:  66% (8/12)   \u001b[K\rremote: Compressing objects:  75% (9/12)   \u001b[K\rremote: Compressing objects:  83% (10/12)   \u001b[K\rremote: Compressing objects:  91% (11/12)   \u001b[K\rremote: Compressing objects: 100% (12/12)   \u001b[K\rremote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 3), reused 13 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:   7% (1/14)   \rUnpacking objects:  14% (2/14)   \rUnpacking objects:  21% (3/14)   \rUnpacking objects:  28% (4/14)   \rUnpacking objects:  35% (5/14)   \rUnpacking objects:  42% (6/14)   \rUnpacking objects:  50% (7/14)   \rUnpacking objects:  57% (8/14)   \rUnpacking objects:  64% (9/14)   \rUnpacking objects:  71% (10/14)   \rUnpacking objects:  78% (11/14)   \rUnpacking objects:  85% (12/14)   \rUnpacking objects:  92% (13/14)   \rUnpacking objects: 100% (14/14)   \rUnpacking objects: 100% (14/14), done.\n",
            "/content/colab-tools\n",
            "Cloning into 'OpenNMT-py'...\n",
            "remote: Enumerating objects: 13531, done.\u001b[K\n",
            "remote: Total 13531 (delta 0), reused 0 (delta 0), pack-reused 13531\u001b[K\n",
            "Receiving objects: 100% (13531/13531), 145.37 MiB | 29.63 MiB/s, done.\n",
            "Resolving deltas: 100% (9640/9640), done.\n",
            "Switched to a new branch 'stable-version'\n",
            "--2019-03-15 16:49:41--  https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.162.216\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.162.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?sUqJLxCzeGyoO-kRpwmifB2ubLHkHdNJoPm_x1kEkjLPZbJQPXDEk3UVqfHf1pfgZtxJn_F2ii6LaQ8tXtvC-hZBEx_A48TEmsXMx2l0VaY-gqPba3qJiTfFXt9wSFdwofTkodbndQAfIaR5m0loki-LaVGzwyqiIZQ8GSlPHnzihvuDFsnWQjGgr6G63XPoOiOh6IOd4ft9uZqto5I_Pg [following]\n",
            "--2019-03-15 16:49:41--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?sUqJLxCzeGyoO-kRpwmifB2ubLHkHdNJoPm_x1kEkjLPZbJQPXDEk3UVqfHf1pfgZtxJn_F2ii6LaQ8tXtvC-hZBEx_A48TEmsXMx2l0VaY-gqPba3qJiTfFXt9wSFdwofTkodbndQAfIaR5m0loki-LaVGzwyqiIZQ8GSlPHnzihvuDFsnWQjGgr6G63XPoOiOh6IOd4ft9uZqto5I_Pg\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.211.70, 2606:2800:21f:3aa:dcf:37b:1ed6:1fb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.211.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1267151038 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.18G   124MB/s    in 9.2s    \n",
            "\n",
            "2019-03-15 16:49:51 (132 MB/s) - ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64’ saved [1267151038/1267151038]\n",
            "\n",
            "\u001b[1;33m\n",
            "WEBGET finished..\n",
            "\u001b[0m\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-2-local.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64 ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-2-local (9.2.148-1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mPo4S4SZHgBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install dependencies used in calma project\n",
        "get_ipython().system('/usr/bin/python3 -m pip install configargparse')\n",
        "get_ipython().system('git clone https://github.com/NIS-2018-CROSS-M/calma_tools.git')\n",
        "\n",
        "# receive the calma\n",
        "get_ipython().system('git clone https://github.com/NIS-2018-CROSS-M/vardial-shared-task')\n",
        "get_ipython().magic('cd vardial-shared-task')\n",
        "get_ipython().system('rm onmt-data/*')\n",
        "get_ipython().system('rm results/*')\n",
        "get_ipython().magic('cd ..')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0kaC6GrfPmc2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A251MlIZHgB8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict as dd\n",
        "from random import shuffle\n",
        "import re\n",
        "import sys\n",
        "sys.path.append(get_ipython().getoutput(\"readlink -e calma_tools\")[0])\n",
        "from calma_tools.ml_util import MLUtil\n",
        "import urllib\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuxvetINHgCE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().magic('cd vardial-shared-task')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lvm-HpdTHgCP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_steps=10000\n",
        "valid_steps=1000\n",
        "save_checkpoint_steps = valid_steps\n",
        "\n",
        "train_params = [\n",
        "    f\"-train_steps {train_steps}\",\n",
        "    f\"-valid_steps {valid_steps}\",\n",
        "    f\"-save_checkpoint_steps {save_checkpoint_steps}\",\n",
        "    f\"-world_size 1\",\n",
        "    f\"-gpu_ranks 0 1\",\n",
        "    f\"-encoder_type brnn\"\n",
        "]\n",
        "\n",
        "pred_params = [\n",
        "    f\"-replace_unk\",\n",
        "    f\"-verbose\",\n",
        "    f\"-n_best 8\",\n",
        "    f\"-beam 8\"\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cBaEw9AHgCX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### data modification"
      ]
    },
    {
      "metadata": {
        "id": "91Uat9GHHgCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataModifyer:\n",
        "    @staticmethod\n",
        "    def modify_src_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_src_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_tgt_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_tgt_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "class NBestDataModifyer:\n",
        "    @staticmethod\n",
        "    def sent_to_baseline_compatible(line):\n",
        "        return line\n",
        "                       \n",
        "    @staticmethod\n",
        "    def hyp_to_baseline_compatible(line):\n",
        "        return line\n",
        "\n",
        "    \n",
        "class DataEvaluator:\n",
        "    otypes = [\"analysis\",\"lemma\",\"tag\"]\n",
        "    \n",
        "    @staticmethod\n",
        "    def update_data(data, line):\n",
        "        lan, wf, lemma, pos, msd = line.split('\\t')\n",
        "        \n",
        "        data[\"analysis\"][wf].add((lemma,pos,msd))\n",
        "        data[\"lemma\"][wf].add(lemma)\n",
        "        data[\"tag\"][wf].add((pos,msd))\n",
        "        \n",
        "        return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qGUbJ2kjHgCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### ml"
      ]
    },
    {
      "metadata": {
        "id": "l06PLGdTHgC0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# filenames, many of them\n",
        "train_uncovered_filename = f\"train/trk-uncovered-transliterated\"\n",
        "train_res_src_filename = f\"onmt-data/trk-src-train.txt\"\n",
        "train_res_tgt_filename = f\"onmt-data/trk-tgt-train.txt\"\n",
        "\n",
        "test_covered_filename = f\"test/trk-covered-transliterated\"\n",
        "\n",
        "\n",
        "test_res_src_filename = f\"onmt-data/trk-src-test.txt\"\n",
        "test_res_tgt_filename = f\"onmt-data/trk-tgt-test.txt\"\n",
        "test_pred_output_filename = f\"results/trk-test-covered.sys\" # output :)\n",
        "\n",
        "val_covered_filename = f\"dev/trk-covered-transliterated\"\n",
        "val_uncovered_filename = f\"dev/trk-uncovered-transliterated\"\n",
        "val_res_src_filename = f\"onmt-data/trk-src-dev.txt\"\n",
        "val_res_tgt_filename = f\"onmt-data/trk-tgt-dev.txt\"\n",
        "val_pred_output_filename = f\"results/trk-dev-covered.sys\" # output :)\n",
        "\n",
        "\n",
        "model_filename = f\"models/trk.model\"\n",
        "\n",
        "score_log_filename = f\"trk-score.log\"\n",
        "\n",
        "def ml(train_params, prediction_params, dataModifyer, nbestModifyer, dataEvaluator):\n",
        "    mlUtil = MLUtil(prediction_params, dataModifyer, nbestModifyer)\n",
        "\n",
        "    get_ipython().system(f'touch {score_log_filename}')\n",
        "\n",
        "\n",
        "    # ml| data preprocessing\n",
        "    mlUtil.generate_data(train_uncovered_filename, train_res_src_filename, train_res_tgt_filename)\n",
        "    mlUtil.generate_data(val_uncovered_filename, val_res_src_filename, val_res_tgt_filename)\n",
        "    mlUtil.generate_data(test_covered_filename, test_res_src_filename, test_res_tgt_filename)\n",
        "\n",
        "    # ml| training\n",
        "    mlUtil.train(train_res_src_filename, train_res_tgt_filename, val_res_src_filename, val_res_tgt_filename, model_filename, train_params)\n",
        "\n",
        "\n",
        "    mlUtil.predict(model_filename, test_res_src_filename, test_covered_filename, test_pred_output_filename)\n",
        "\n",
        "\n",
        "    mlUtil.predict(model_filename, val_res_src_filename, val_covered_filename, val_pred_output_filename)\n",
        "\n",
        "    get_ipython().system(f'echo \"*===QUALITY ON VAL DATA===*\" >> {score_log_filename}')\n",
        "    mlUtil.score_predictions(val_pred_output_filename, val_uncovered_filename, score_log_filename, dataEvaluator)\n",
        "\n",
        "    # log eval results\n",
        "    get_ipython().system(f'cat {score_log_filename}')\n",
        "\n",
        "    # send eval to @oserikov at telegram\n",
        "    if telegram_notifications_enabled:\n",
        "        telegram_message = f\"#score\\n{lang}\\n{track}\\n\"+''.join(open(score_log_filename).readlines())+'\\n'+EXP_DESCRIPTION\n",
        "\n",
        "        telegram_message_encoded = urllib.parse.quote(telegram_message)\n",
        "        get_ipython().system(f'curl -i -X GET \"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={chat_id}&text={telegram_message_encoded}&parse_mode=markdown\"')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZijMxhu6k_Sk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!head {val_pred_output_filename}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7IrK1ezHgC9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ml(train_params, pred_params, TrainDataModifyer, NBestDataModifyer, DataEvaluator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GG_2F8mCHgDC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  # sandbox"
      ]
    },
    {
      "metadata": {
        "id": "2uxBYr92C3sQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !!! mlUtil.score_predictions(val_pred_output_filename, val_uncovered_filename, score_log_filename, dataEvaluator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "51iDM8gO125a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(val_pred_output_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}