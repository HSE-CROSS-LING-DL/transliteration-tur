{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "calma_crh_pos.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "f5r3sVENlfb_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### set up telegram notifications\n",
        "\n",
        "  не очень понятно, нужно ли это.\n",
        "\n",
        "  если нужно -- напишите @oserikov в телеграме, я расскажу, что сделать,\n",
        "  чтобы присылались сообщения с качеством модели когда она отработает."
      ]
    },
    {
      "metadata": {
        "id": "xNb8dZi9lfcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "telegram_notifications_enabled=False\n",
        "EXP_DESCRIPTION = \"PREDICT ONLY POS\"\n",
        "\n",
        "if telegram_notifications_enabled:\n",
        "    bot_token = input(\"введите telegram bot token: \")\n",
        "    chat_id = \"292749902\" # for @oserikov\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCruX1TXlfcP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### install prereqs"
      ]
    },
    {
      "metadata": {
        "id": "JoU_sFhnlfcS",
        "colab_type": "code",
        "outputId": "0a20d6b3-f079-4fa6-b1a0-da8eafd99bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "cell_type": "code",
      "source": [
        "get_ipython().system(f\"git clone https://github.com/NIS-2018-CROSS-M/colab-tools.git\")\n",
        "get_ipython().magic(f\"cd colab-tools\")\n",
        "get_ipython().system(f\"bash colab-install-opennmt.sh\")\n",
        "get_ipython().system(f\"bash colab-install-cuda92-pytorch41.sh\")\n",
        "get_ipython().system(f\"bash colab-install-torchtext.sh\")\n",
        "get_ipython().magic(f\"cd ..\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab-tools'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 3), reused 13 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n",
            "/content/colab-tools\n",
            "Cloning into 'OpenNMT-py'...\n",
            "remote: Enumerating objects: 13531, done.\u001b[K\n",
            "remote: Total 13531 (delta 0), reused 0 (delta 0), pack-reused 13531\n",
            "Receiving objects: 100% (13531/13531), 145.37 MiB | 33.03 MiB/s, done.\n",
            "Resolving deltas: 100% (9639/9639), done.\n",
            "Switched to a new branch 'stable-version'\n",
            "--2019-03-13 12:25:55--  https://developer.nvidia.com/compute/cuda/9.2/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.162.216\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.162.216|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?fyNmpQaSP2tsGngALUvF2UPL7Vf3zyKhJKhUh5eZz-h1fXlvmy0YThiPSEvglco9cGAdyZqjPjzwF7B3268zBjVowylIjfOohuqqupW7bx4bbm7kfY0hcytSDzSIJeIAvBtZ_rIeUoRADX1GxG0plGw2i9p1Thhmz9yWJoNm7jtxbnxVAQ11cdD2em1QPFK0m100mduyZqK3ISa8JU2j2w [following]\n",
            "--2019-03-13 12:25:55--  https://developer.download.nvidia.com/compute/cuda/9.2/secure/Prod2/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64.deb?fyNmpQaSP2tsGngALUvF2UPL7Vf3zyKhJKhUh5eZz-h1fXlvmy0YThiPSEvglco9cGAdyZqjPjzwF7B3268zBjVowylIjfOohuqqupW7bx4bbm7kfY0hcytSDzSIJeIAvBtZ_rIeUoRADX1GxG0plGw2i9p1Thhmz9yWJoNm7jtxbnxVAQ11cdD2em1QPFK0m100mduyZqK3ISa8JU2j2w\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.211.70, 2606:2800:21f:3aa:dcf:37b:1ed6:1fb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.211.70|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1267151038 (1.2G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-2-local_9.2.148-1_amd64’\n",
            "\n",
            "    cuda-repo-ubunt  16%[==>                 ] 202.75M   156MB/s               "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dgxvGLj1lfca",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# install dependencies used in calma project\n",
        "get_ipython().system('/usr/bin/python3 -m pip install configargparse')\n",
        "get_ipython().system('git clone https://github.com/NIS-2018-CROSS-M/calma_tools.git')\n",
        "\n",
        "# receive the calma\n",
        "get_ipython().system('git clone https://github.com/NIS-2018-CROSS-M/vardial-shared-task')\n",
        "get_ipython().magic('cd vardial-shared-task')\n",
        "get_ipython().system('rm onmt-data/*')\n",
        "get_ipython().system('rm results/*')\n",
        "get_ipython().magic('cd ..')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DgTDxpGrlfcg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(get_ipython().getoutput(\"readlink -e calma_tools\")[0])\n",
        "from calma_tools.ml_util import MLUtil\n",
        "import urllib\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nCRMhA24lfcl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "get_ipython().magic('cd vardial-shared-task')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LulvG_bulfcv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_steps=10000\n",
        "valid_steps=1000\n",
        "save_checkpoint_steps = valid_steps\n",
        "\n",
        "train_params = [\n",
        "    f\"-train_steps {train_steps}\",\n",
        "    f\"-valid_steps {valid_steps}\",\n",
        "    f\"-save_checkpoint_steps {save_checkpoint_steps}\",\n",
        "    f\"-world_size 1\",\n",
        "    f\"-gpu_ranks 0 1\",\n",
        "    f\"-encoder_type brnn\"\n",
        "]\n",
        "\n",
        "pred_params = [\n",
        "    f\"-replace_unk\",\n",
        "    f\"-verbose\",\n",
        "    f\"-n_best 8\",\n",
        "    f\"-beam 8\"\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OzlXLclPlfcz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### data modification"
      ]
    },
    {
      "metadata": {
        "id": "X6l39hanlfc3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TrainDataModifyer:\n",
        "    @staticmethod\n",
        "    def modify_src_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_src_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_tgt_line(line):\n",
        "        return '+' + line.split('+')[1].rstrip(' ')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def restore_tgt_line(line):\n",
        "        return line\n",
        "\n",
        "\n",
        "class NBestDataModifyer:\n",
        "    @staticmethod\n",
        "    def sent_to_baseline_compatible(line):\n",
        "        return line\n",
        "\n",
        "    @staticmethod\n",
        "    def hyp_to_baseline_compatible(line):\n",
        "        line_splitted = line.split('] [')\n",
        "        line_splitted[1] = line_splitted[1].rstrip(']')  # (line_splitted[1].split(']')[0])\n",
        "        if len(line_splitted) < 2 or line_splitted[1] == \"\":\n",
        "            line_splitted[1] = '\\'+NOUN\\''\n",
        "        return line_splitted[0] + '] [' + ', '.join(['\\'c\\'','\\'c\\'', line_splitted[1], '\\'+Tag1=Value1\\'', '\\'+Tag2=Value2\\'', '\\'+Language=lan\\'']) + ']'\n",
        "\n",
        "\n",
        "class DataEvaluator:\n",
        "    otypes = [\"pos-tag\"]\n",
        "\n",
        "    @staticmethod\n",
        "    def update_data(data, line):\n",
        "        lan, wf, lemma, pos, msd = line.split('\\t')\n",
        "\n",
        "        data[\"pos-tag\"][wf].add(pos)\n",
        "        \n",
        "        return data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-aNgOaa2lfc-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  #### ml"
      ]
    },
    {
      "metadata": {
        "id": "kmTPKI4Ilfc_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# filenames, many of them\n",
        "train_uncovered_filename = f\"train/trk-uncovered-transliterated\"\n",
        "train_res_src_filename = f\"onmt-data/trk-src-train.txt\"\n",
        "train_res_tgt_filename = f\"onmt-data/trk-tgt-train.txt\"\n",
        "\n",
        "test_covered_filename = f\"test/trk-covered-transliterated\"\n",
        "\n",
        "\n",
        "test_res_src_filename = f\"onmt-data/trk-src-test.txt\"\n",
        "test_res_tgt_filename = f\"onmt-data/trk-tgt-test.txt\"\n",
        "test_pred_output_filename = f\"results/trk-test-covered.sys\" # output :)\n",
        "\n",
        "val_covered_filename = f\"dev/trk-covered-transliterated\"\n",
        "val_uncovered_filename = f\"dev/trk-uncovered-transliterated\"\n",
        "val_res_src_filename = f\"onmt-data/trk-src-dev.txt\"\n",
        "val_res_tgt_filename = f\"onmt-data/trk-tgt-dev.txt\"\n",
        "val_pred_output_filename = f\"results/trk-dev-covered.sys\" # output :)\n",
        "\n",
        "\n",
        "model_filename = f\"models/trk.model\"\n",
        "\n",
        "score_log_filename = f\"trk-score.log\"\n",
        "\n",
        "\n",
        "def ml(train_params, prediction_params, dataModifyer, nbestModifyer, dataEvaluator):\n",
        "    mlUtil = MLUtil(prediction_params, dataModifyer, nbestModifyer)\n",
        "\n",
        "\n",
        "    get_ipython().system(f'touch {score_log_filename}')\n",
        "\n",
        "\n",
        "    # ml| data preprocessing\n",
        "    mlUtil.generate_data(train_uncovered_filename, train_res_src_filename, train_res_tgt_filename)\n",
        "    mlUtil.generate_data(val_uncovered_filename, val_res_src_filename, val_res_tgt_filename)\n",
        "    mlUtil.generate_data(test_covered_filename, test_res_src_filename, test_res_tgt_filename)\n",
        "\n",
        "    # ml| training\n",
        "    mlUtil.train(train_res_src_filename, train_res_tgt_filename, val_res_src_filename, val_res_tgt_filename, model_filename, train_params)\n",
        "\n",
        "    # ml| predict for test\n",
        "    mlUtil.predict(model_filename, test_res_src_filename, test_covered_filename, test_pred_output_filename)\n",
        "\n",
        "    # ml| predict and eval for val\n",
        "    mlUtil.predict(model_filename, val_res_src_filename, val_covered_filename, val_pred_output_filename)\n",
        "\n",
        "\n",
        "    get_ipython().system(f'echo \"*===QUALITY ON VAL DATA===*\" >> {score_log_filename}')\n",
        "    mlUtil.score_predictions(val_pred_output_filename, val_uncovered_filename, score_log_filename, dataEvaluator)\n",
        "\n",
        "    # log eval results\n",
        "    get_ipython().system(f'cat {score_log_filename}')\n",
        "\n",
        "    # send eval to @oserikov at telegram\n",
        "    if telegram_notifications_enabled:\n",
        "        telegram_message = f\"#score\\n{lang}\\n{track}\\n\"+''.join(open(score_log_filename).readlines())+'\\n'+EXP_DESCRIPTION\n",
        "\n",
        "        telegram_message_encoded = urllib.parse.quote(telegram_message)\n",
        "        get_ipython().system(f'curl -i -X GET \"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={chat_id}&text={telegram_message_encoded}&parse_mode=markdown\"')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bP0m3tmVlfdK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ml(train_params, pred_params, TrainDataModifyer, NBestDataModifyer, DataEvaluator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "maB7a5F0lfdP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  # sandbox"
      ]
    },
    {
      "metadata": {
        "id": "6rvOzNhWuNxr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(val_pred_output_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}